# -- Override the deployment namespace
namespaceOverride: "olake"

# -- Provide a name in place of olake for `app:` labels
nameOverride: ""

# -- Provide a name to substitute for the full names of resources
fullnameOverride: ""

# -- Global configuration that applies to all components
global:
  # -- Global storage class for all persistent volumes
  # Set to "" to disable dynamic provisioning
  # Examples: gp2, standard, managed
  storageClass: ""
  
  # -- Job scheduling configuration for pods created by olake-worker
  # Controls how sync, discover, and test job pods are scheduled on the cluster
  job:
    sync:
      nodeSelector: {}
      tolerations: []
      antiAffinity:
        enabled: true
        strategy: "hard"
        topologyKey: "kubernetes.io/hostname"
    
    discover:
      nodeSelector: {}
      tolerations: []
      antiAffinity:
        enabled: false
        strategy: "soft"
        topologyKey: "kubernetes.io/hostname"
        weight: 10
    
    test:
      nodeSelector: {}
      tolerations: []
      antiAffinity:
        enabled: false
        strategy: "soft"
        topologyKey: "kubernetes.io/hostname"
        weight: 10

# =============================================================================
# OLAKE UI
# =============================================================================

# OLake UI - Main application frontend and backend
olakeUI:
  # -- Enable OLake UI deployment
  enabled: true
  
  # -- Number of OLake UI replicas
  # For production, consider 2+ replicas for high availability
  replicaCount: 1
  
  # -- OLake UI image configuration
  image:
    repository: olakego/olake-ui
    tag: latest
    pullPolicy: Always
  
  # -- Pod scheduling constraints
  nodeSelector: {}
  
  # -- Tolerations for pod assignment
  tolerations: []
  
  # -- Affinity settings for pod assignment
  affinity: {}
  
  # -- Additional pod annotations
  podAnnotations: {}
  
  # -- Additional pod labels
  podLabels: {}
  
  # -- OLake UI service configuration
  # For external access, change type to NodePort or LoadBalancer  
  service:
    # -- Service type (ClusterIP for internal, NodePort/LoadBalancer for external access)
    type: ClusterIP
    annotations: {}
  
  # -- OLake UI resource requirements (optional overrides)
  # Defaults: requests: memory=512Mi, cpu=500m (no limits by default)
  # Example customizations:
  #   resources:
  #     requests:
  #       memory: "1Gi"
  #       cpu: "750m"
  #     limits:
  #       memory: "2Gi"
  #       cpu: "1500m"
  resources: {}
  
  # -- Ingress configuration for external access
  ingress:
    # -- Enable ingress
    enabled: false
    
    # -- Ingress class name
    # Examples: nginx, traefik, alb
    className: ""
    
    # -- Ingress annotations
    # Add custom annotations for your ingress controller here.
    # Examples:
    #   nginx.ingress.kubernetes.io/rewrite-target: /
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: "true"
    #   cert-manager.io/cluster-issuer: "letsencrypt-prod"
    annotations: {}
    
    # -- Ingress hosts configuration
    # List of host definitions for ingress. Each host can have multiple paths.
    # Examples:
    #   hosts:
    #     - host: olake.local
    #       paths:
    #         - path: /
    #           pathType: Prefix
    #     - host: olake.example.com
    #       paths:
    #         - path: /
    #           pathType: Prefix
    hosts: []
    
    # -- TLS configuration
    # Examples:
    #   tls:
    #     - secretName: olake-tls
    #       hosts:
    #         - olake.example.com
    tls: []
  
  # -- Initialization job configuration
  initJob:
    # -- Admin user credentials for initial setup
    adminUser:
      # -- Admin username
      username: "admin"
      
      # -- Admin password (change this in production!)
      password: "password"
      
      # -- Admin email address
      email: "admin@example.com"

# =============================================================================
# OLAKE WORKER
# =============================================================================

# OLake K8s Worker - Executes data pipeline tasks as Kubernetes pods
olakeWorker:
  # -- Enable OLake K8s Worker deployment
  enabled: true
  
  # -- Number of OLake Worker replicas
  # Multiple replicas can handle more concurrent workflows
  replicaCount: 1
  
  # -- OLake Worker image configuration
  image:
    repository: olakego/olake-worker-k8s
    tag: latest
    pullPolicy: Always
  
  # -- Pod scheduling constraints
  nodeSelector: {}
  
  # -- Tolerations for pod assignment
  tolerations: []
  
  # -- Affinity settings for pod assignment
  affinity: {}
  
  # -- Additional pod annotations
  podAnnotations: {}
  
  # -- Additional pod labels
  podLabels: {}
  
  # -- OLake Worker resource requirements (optional overrides)
  # Defaults: requests: memory=256Mi, cpu=250m (no limits by default)
  # Example customizations:
  #   resources:
  #     requests:
  #       memory: "512Mi"
  #       cpu: "500m"
  #     limits:
  #       memory: "1Gi"
  #       cpu: "1000m"
  resources: {}
  
  # -- Service Account configuration for Kubernetes API access
  serviceAccount:
    # -- Create service account
    create: true
    
    # -- Service account name
    # If not set and create is true, a name is generated using the fullname template
    name: ""
    
    # -- Service account annotations
    annotations: {}
  
  # -- OLake Worker application configuration
  config:
    worker:
      # -- Maximum concurrent activities(https://docs.temporal.io/activities) per worker
      # Increase for higher throughput, decrease for resource constraints
      maxConcurrentActivities: 15
      
      # -- Maximum concurrent workflows(https://docs.temporal.io/workflows) per worker
      maxConcurrentWorkflows: 10
    
    # -- Timeout configuration for different operation types
    timeouts:
      # -- Workflow execution timeouts
      workflow:
        discover: 3h
        test: 3h
        sync: 720h

      # -- Activity execution timeouts
      activity:
        discover: 2h
        test: 2h
        sync: 700h
    
    # -- Logging configuration
    logging:
      level: info
      format: json

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================

# Storage configuration for shared data exchange between pods
# Choose between self-managed NFS server or external storage (EFS/Azure Files)
nfsServer:
  # -- Enable self-managed NFS server deployment
  # When false, you must provide external storage (EFS/Azure Files/etc.)
  enabled: true

  # -- External NFS server configuration (used when enabled: false)
  # When disabled, you must provide an existing ReadWriteMany (RWX) PersistentVolumeClaim
  # 
  # CLOUD PROVIDER GUIDES FOR RWX STORAGE:
  # 
  # AWS EFS: Use the Amazon EFS CSI driver with 'efs.csi.aws.com' provisioner
  # See: https://github.com/kubernetes-sigs/aws-efs-csi-driver
  # 
  # Azure Files: Use 'azurefile' StorageClass with 'kubernetes.io/azure-file' provisioner  
  # See: https://docs.microsoft.com/en-us/azure/aks/azure-files-csi
  # 
  # GCP Filestore: Use Google Cloud Filestore CSI driver with 'filestore.csi.storage.gke.io' provisioner
  # See: https://cloud.google.com/filestore/docs/csi-driver
  # 
  # STEPS TO CREATE RWX PVC:
  # 1. Find RWX StorageClass: kubectl get sc
  # 2. Create PVC with accessModes: [ReadWriteMany] and your RWX storageClassName
  # 3. Reference the PVC name in the 'name' field below
  external:
    # -- The name of the PersistentVolumeClaim for the external RWX storage
    # Example: "efs-pvc" or "azure-files-pvc" or "gcp-filestore-pvc"
    name: ""
  
  # -- Static cluster IP for NFS server service
  # Must be within your cluster's service CIDR range and not already in use
  # 
  # HOW TO FIND YOUR CLUSTER'S SERVICE CIDR:
  # Method 1 (Most clusters): kubectl get pod $(kubectl get pods -n kube-system -l component=kube-apiserver -o jsonpath='{.items[0].metadata.name}') -n kube-system -o yaml | grep service-cluster-ip-range
  # Method 2 (GKE): gcloud container clusters describe CLUSTER_NAME --zone ZONE --format 'value(servicesIpv4CidrBlock)'
  # Method 3 (AKS): az aks show --resource-group RG --name CLUSTER_NAME --query 'networkProfile.serviceCidr' -o tsv
  # Method 4 (EKS): kubectl get service kubernetes -o jsonpath='{.spec.clusterIP}' (infer CIDR from this IP)
  #
  # CHECK IF IP IS FREE: kubectl get services --all-namespaces | grep -w "YOUR_IP"
  # (No output means the IP is available)
  #
  # BEST PRACTICES:
  # - Choose from the higher end of the CIDR range (e.g., .250, .100, .50)
  # - Avoid the very beginning of the range (reserved for core services)
  # - Document the choice to prevent conflicts
  #
  # Examples by common CIDR ranges:
  # - 10.96.0.0/12 → use 10.96.0.50 or 10.100.0.50
  # - 10.0.0.0/16 → use 10.0.255.50
  # - 172.20.0.0/16 → use 172.20.255.50
  clusterIP: ""
  
  # -- NFS server image configuration
  image:
    repository: itsthenetwork/nfs-server-alpine
    tag: latest
    pullPolicy: Always
  
  # -- Pod scheduling constraints
  nodeSelector: {}
  
  # -- Tolerations for pod assignment
  tolerations: []
  
  # -- Affinity settings for pod assignment
  affinity: {}
  
  # -- Additional pod annotations
  podAnnotations: {}
  
  # -- Additional pod labels
  podLabels: {}
  
  # -- NFS server persistence configuration
  # This is the underlying storage for the NFS server itself
  persistence:
    # -- Size of underlying storage for NFS server
    size: 20Gi

    # -- Storage class for underlying storage
    storageClass: ""
    
    # -- Access modes for underlying storage
    accessModes:
      - ReadWriteOnce

# =============================================================================
# TEMPORAL WORKFLOW ENGINE
# =============================================================================

# Temporal configuration for workflow orchestration
temporal:
  # -- Enable Temporal deployment
  enabled: true
  
  # -- Temporal server configuration
  server:
    # -- Number of Temporal server replicas
    replicaCount: 1
    
    # -- Temporal server image configuration
    image:
      repository: temporalio/auto-setup
      tag: "1.22.3"
      pullPolicy: Always
    
    # -- Pod scheduling constraints
    nodeSelector: {}
    
    # -- Tolerations for pod assignment
    tolerations: []
    
    # -- Affinity settings for pod assignment
    affinity: {}
    
    # -- Additional pod annotations
    podAnnotations: {}
    
    # -- Additional pod labels
    podLabels: {}
    
    # -- Temporal server resource requirements (optional overrides)
    # Defaults: requests: memory=512Mi, cpu=500m (no limits by default)
    # Example customizations:
    #   resources:
    #     requests:
    #       memory: "1Gi"
    #       cpu: "750m"
    #     limits:
    #       memory: "2Gi"
    #       cpu: "1500m"
    resources: {}
  
  # -- Temporal Web UI configuration (disabled by default)
  ui:
    # -- Enable Temporal Web UI
    enabled: false

# =============================================================================
# POSTGRESQL DATABASE
# =============================================================================

# PostgreSQL is used as the backend database for Temporal and OLake
postgresql:
  # -- Enable PostgreSQL deployment
  enabled: true
  
  # -- Number of PostgreSQL replicas
  # Note: PostgreSQL clustering requires additional configuration
  replicaCount: 1
  
  # -- PostgreSQL image configuration
  image:
    repository: postgres
    tag: "14-alpine"
    pullPolicy: Always
  
  # -- Authentication configuration
  auth:
    # -- PostgreSQL super user name
    postgresUser: "temporal"
    
    # -- PostgreSQL super user password
    # In production, consider using existingSecret
    postgresPassword: "temporal"
    
    # -- PostgreSQL application database
    database: "temporal"
    
    # -- Name of existing secret containing PostgreSQL credentials
    # Keys: postgres-password, password, username
    existingSecret: ""
    
    # -- Key in existing secret containing PostgreSQL super user password
    secretKeys:
      adminPasswordKey: postgres-password
      userPasswordKey: password
      usernameKey: username
  
  # -- Pod scheduling constraints
  nodeSelector: {}
  
  # -- Tolerations for pod assignment
  tolerations: []
  
  # -- Affinity settings for pod assignment
  affinity: {}
  
  # -- Additional pod annotations
  podAnnotations: {}
  
  # -- Additional pod labels
  podLabels: {}
  
  # -- PostgreSQL resource requirements (optional overrides)
  # Defaults: requests: memory=256Mi, cpu=250m (no limits by default)
  # Example customizations:
  #   resources:
  #     requests:
  #       memory: "512Mi"
  #       cpu: "500m"
  #     limits:
  #       memory: "1Gi"
  #       cpu: "1000m"
  resources: {}
  
  # -- PostgreSQL persistence configuration
  persistence:
    # -- Enable persistent storage
    enabled: true
    
    # -- Size of persistent volume
    size: 8Gi
    
    # -- Access modes for persistent volume
    accessModes:
      - ReadWriteOnce
    
    # -- Additional annotations for PVC
    annotations: {}

# =============================================================================
# ELASTICSEARCH
# =============================================================================

# Elasticsearch is used by Temporal for advanced visibility features
elasticsearch:
  # -- Enable Elasticsearch deployment
  # Set to false to disable advanced visibility in Temporal
  enabled: true
  
  # -- Number of Elasticsearch replicas
  replicaCount: 1
  
  # -- Elasticsearch image configuration
  image:
    repository: elasticsearch
    tag: "7.17.10"
    pullPolicy: Always
  
  # -- Pod scheduling constraints
  nodeSelector: {}
  
  # -- Tolerations for pod assignment
  tolerations: []
  
  # -- Affinity settings for pod assignment
  affinity: {}
  
  # -- Additional pod annotations
  podAnnotations: {}
  
  # -- Additional pod labels
  podLabels: {}
  
  # -- Elasticsearch resource requirements (optional overrides)
  # Defaults: requests: memory=1Gi, cpu=500m (no limits by default)
  # Example customizations:
  #   resources:
  #     requests:
  #       memory: "2Gi"
  #       cpu: "1000m"
  #     limits:
  #       memory: "4Gi"
  #       cpu: "2000m"
  resources: {}
  
